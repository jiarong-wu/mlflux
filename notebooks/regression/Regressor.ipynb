{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import ngboost\n",
    "from ngboost.distns import MultivariateNormal\n",
    "from ngboost.scores import LogScore\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class predictor:\n",
    "    \"\"\"\n",
    "    Attributes inherited from all classes\n",
    "    __init__ fills the class based on a dictionary\n",
    "    save pickles itself, this is for convience and one does not need to convert\n",
    "    self.fname to a filename etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params={}):\n",
    "        for key in params.keys():\n",
    "            setattr(self, key, params[key])\n",
    "\n",
    "    def save(self, fname=\"model_1\"):\n",
    "        ## Use fname if the fname is defined.\n",
    "        pickle.dump(self, open(getattr(self, \"fname\", fname) + \".p\", \"wb\"))\n",
    "        \n",
    "\n",
    "class mvn_predictor(predictor):\n",
    "    \"\"\"\n",
    "    Template, nothing defined as mvn predictors have custom treatments\n",
    "    \"\"\"\n",
    "\n",
    "    def summary(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_BASE = {\n",
    "    \"criterion\": \"friedman_mse\",\n",
    "    \"min_samples_leaf\": 1,\n",
    "    \"min_samples_split\": 2,\n",
    "    \"min_weight_fraction_leaf\": 0.0,\n",
    "    \"max_depth\": 3,\n",
    "    \"splitter\": \"best\",\n",
    "}\n",
    "\n",
    "DEFAULT_NGBOOST = {\"minibatch_frac\": 1.0, \"learning_rate\": 0.01}\n",
    "\n",
    "MAX_ITER = 1000\n",
    "\n",
    "class mvn_ngboost(mvn_predictor):\n",
    "    def __init__(self, params={}):\n",
    "        super().__init__(params)\n",
    "        self.model = None\n",
    "        self.base = None\n",
    "        if not hasattr(self, \"params_base\"):\n",
    "            self.params_base = DEFAULT_BASE\n",
    "        if not hasattr(self, \"params_ngboost\"):\n",
    "            self.params_ngboost = DEFAULT_NGBOOST\n",
    "\n",
    "    def fit(self, X_train, Y_train, X_valid, Y_valid, base=None, retrain=False):\n",
    "        p = Y_train.shape[1]\n",
    "        dist = MultivariateNormal(p)\n",
    "\n",
    "        if base is None:\n",
    "            b2 = DecisionTreeRegressor(**self.params_base)\n",
    "            self.base = b2\n",
    "            print(self.params_base)\n",
    "        else:\n",
    "            b2 = base\n",
    "            self.base = base\n",
    "        n_estimators = MAX_ITER\n",
    "        self.model = ngboost.NGBoost(\n",
    "            Dist=dist,\n",
    "            Score=LogScore,\n",
    "            Base=b2,\n",
    "            **self.params_ngboost,\n",
    "            verbose_eval=10,\n",
    "            n_estimators=n_estimators\n",
    "        )\n",
    "\n",
    "        self.model.fit(\n",
    "            X=X_train,\n",
    "            Y=Y_train,\n",
    "            X_val=X_valid,\n",
    "            Y_val=Y_valid,\n",
    "            early_stopping_rounds=100,\n",
    "        )\n",
    "        self.model.best_val_loss_itr\n",
    "        if retrain:\n",
    "            self.model = ngboost.NGBoost(\n",
    "                Dist=dist,\n",
    "                Score=LogScore,\n",
    "                Base=b2,\n",
    "                **self.params_ngboost,\n",
    "                verbose_eval=10,\n",
    "                n_estimators=self.model.best_val_loss_itr + 1\n",
    "            )\n",
    "\n",
    "            X = np.vstack([X_train, X_valid])\n",
    "            Y = np.vstack([Y_train, Y_valid])\n",
    "            self.model.fit(X=X, Y=Y)\n",
    "\n",
    "    def scipy_distribution(self, X_test, cmat_output=False):\n",
    "        preds = self.model.pred_dist(X_test, max_iter=self.model.best_val_loss_itr)\n",
    "        if cmat_output:\n",
    "            out = [preds.mean(), preds.cov]\n",
    "        else:\n",
    "            out = preds.scipy_distribution()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-do: write a function to check residual norm\n",
    "\n",
    "residual_norm = (Y_test[:,1] - Y_dists.mean())/Y_dists.std()\n",
    "print(np.mean(residual_norm), np.std(residual_norm))\n",
    "standard_normal = np.random.randn(residual_norm.size)\n",
    "plt.hist(standard_normal, bins=50, alpha=0.5, density=True)\n",
    "plt.hist(residual_norm, bins=50, alpha=0.5, density=True)\n",
    "plt.show()\n",
    "# plt.yscale('log')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflux",
   "language": "python",
   "name": "mlflux"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
